{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "553c9aef-4023-493d-bce5-49b1b4a794a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6134090b-3d0a-40e7-904a-4b4f5c34773b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Login with credentials to the App \n",
    "#Reddit API\n",
    "reddit = praw.Reddit(\n",
    "client_id = 'KEogXJHap1nKNfd1u6IpNA',\n",
    "client_secret='5dOZZ63Wi57e3YaGNIHJDLAu3pzrjg',   \n",
    "user_agent = 'my-app by u/Competitive-Shock179'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852e7688-c2f8-40be-8db5-02dbc54d541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6aa120eb-1dc5-4c99-9a09-a678f95eb6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code section to test whether for the keywords in the list with the given subreddits, all the relevant values with their\n",
    "#respective columns gets added to the WBlist\n",
    "\n",
    "WBlist = []\n",
    "\n",
    "#Searching the subriddit first in tech subreddit, then technology subreddit and so on...\n",
    "subreddit = reddit.subreddit('technology')\n",
    "\n",
    "#Keywords related to Tech and Whisteblowing\n",
    "keywords = [\n",
    "    'Facebook_whistleblower', 'Facebook_whistleblow', 'Facebook_whistleblowing',\n",
    "    'Twitter_whistleblow', 'twitter_whistleblowing', 'Twitter_whistleblower',\n",
    "    'google_whistleblow', 'google_whistleblowing', 'Google_whistleblower',\n",
    "    'Instagram_whistleblow', 'instagram_whistleblowing', 'Instagram_whistleblower',\n",
    "    'Meta_whistleblow', 'meta_whistleblowing', 'Meta_whistleblower',\n",
    "    'Alphabet_whistleblow', 'alphabet_whistleblowing', 'Alphabet_whistleblower',\n",
    "    'Netflix_whistleblow', 'netflix_whistleblowing', 'Netflix_whistleblower',\n",
    "    'Amazon_whistleblower', 'Amazon_whistleblow', 'Amazon_whistleblowing',\n",
    "    'Microsoft_whistleblow', 'microsoft_whistleblowing', 'Microsoft_whistleblower',\n",
    "    'Tech_whistleblow', 'tech_whistleblowing', 'Tech_whistleblower',\n",
    "    'Software_whistleblow', 'software_whistleblowing', 'Software_whistleblower',\n",
    "    'Whistleblow_software_industry', 'Whistleblowing_software_industry', 'Whistleblower_software_industry',\n",
    "    'Uber_whistleblower', 'Uber_whistleblow', 'Uber_whistleblowing',\n",
    "    'Tiktok_whistleblower', 'Tiktok_whistleblow', 'Tiktok_whistleblowing',\n",
    "    'Apple_whistleblower', 'Apple_whistleblow', 'apple_whistleblowing'\n",
    "]\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "    for submission in subreddit.search(keyword, time_filter='all', limit=None):\n",
    "        WBlist.append(submission.id)\n",
    "        WBlist.append(submission.title)\n",
    "        WBlist.append(submission.num_comments)\n",
    "        WBlist.append(submission.subreddit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7346cf-0750-4b1a-b397-81c65b879e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WBlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c07a9-fae0-4927-b295-0173067f960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(WBlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508ccff-b481-4879-a1c0-1c53b8b3a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code section to search all the posts in the given subreddit list 'subreddits'\n",
    "#This code collects all the posts in one go from the subreddits and not in Batches\n",
    "data = []\n",
    "dataFramePosts = []\n",
    "postIDs = []\n",
    "\n",
    "#File containing all the relevant keywords related to whistleblowing\n",
    "with open('Keywords_list_WB.txt', 'r') as file:\n",
    "   lines = file.readlines()\n",
    "\n",
    "WhistleblowingWords = [line.strip() for line in lines]\n",
    "\n",
    "subreddits = ['technology','tech','technews','worldnews','news','programming','privacy','politics','google','cybersecurity']  \n",
    "\n",
    "for subreddititem in subreddits:\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddititem)\n",
    "\n",
    "    for keywordWB in WhistleblowingWords:            \n",
    "            creationdateStr = datetime.utcfromtimestamp(int(post.created_utc)).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "            for post in subreddit.search(keywordWB, time_filter='all', limit=None):\n",
    "                dataFramePosts.append([post.id, post.title, post.selftext, post.num_comments,creationdateStr,post.subreddit])\n",
    "\n",
    "            ResDFrame = pd.DataFrame(dataFramePosts,columns=['id','title','selftext','num_comments','creationdatestr','subreddit'])\n",
    "            ResDFrame = ResDFrame.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "084e169d-0024-4ff1-b1da-3f05a1e70e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This code collects all the posts in batches by manually specin order to not encounter TooManyRequests: received 429 HTTP response\n",
    "data = []\n",
    "dataFramePosts = []\n",
    "postIDs = []\n",
    "\n",
    "#File containing all the relevant keywords related to whistleblowing\n",
    "with open('Keywords_list_WB.txt', 'r') as file:\n",
    "   lines = file.readlines()\n",
    "\n",
    "WhistleblowingWords = [line.strip() for line in lines]\n",
    "\n",
    "subreddit = reddit.subreddit('technology') #,'tech','technews','worldnews','news','programming','privacy','politics','google','cybersecurity'] \n",
    "\n",
    "for keywordWB in WhistleblowingWords:            \n",
    "        creationdateStr = datetime.utcfromtimestamp(int(post.created_utc)).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "        for post in subreddit.search(keywordWB, time_filter='all', limit=None):\n",
    "            dataFramePosts.append([post.id, post.title, post.selftext, post.num_comments,creationdateStr,post.subreddit])\n",
    "\n",
    "        ResDFrame = pd.DataFrame(dataFramePosts,columns=['id','title','selftext','num_comments','creationdatestr','subreddit'])\n",
    "        ResDFrame = ResDFrame.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ed27c-f480-4488-9b9d-cd1803403191",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResDFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1802038c-352d-4e71-8b83-183d146312b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResDFrame.to_csv(\"Whistleblowingdata_posts1.csv\", index=False) #Writing the collected posts into a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "9e410b73-86bc-4579-b991-0aa7f6f63387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CodeSection that generates a random subset of a Pandas Dataframe.\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def random_subset(dataframe, size):\n",
    "\n",
    "    if size > len(dataframe):\n",
    "        raise ValueError(\"The subset size is larger than DataFrame size.\")\n",
    "\n",
    "    indices = random.sample(range(len(dataframe)), size)\n",
    "    \n",
    "    subset = dataframe.iloc[indices]\n",
    "    \n",
    "    subset.to_csv(\"randomsampledataframe.csv\", index=False)\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66e8e1-7b1d-40c1-9129-e537e1ce5723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_subset(dataframe,353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae0a14-dcab-4244-951b-3a4393a55a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dfPost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f3019-410a-433a-b9a9-bda320a311e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(set(dfPost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9988df7-6d99-4393-b91b-7cb19f6a3e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResDFrame.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "607c0d67-361f-45b3-9655-f4f5f2f92650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_wistleblowing = pd.read_csv('WBDatasetstec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "dd37247a-4c1b-42c9-bb34-f422f0ff2188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_wistleblowing = dataframe_wistleblowing.drop_duplicates(subset = \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab3cc6-5433-496a-b0ca-9a93a2641a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_wistleblowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "626242eb-f100-4308-ba93-d4ecfc82f672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('WBDatasetstec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd17df-a465-4007-9b7b-c685c1038ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "ae6f7d26-ea68-4bc1-804a-6c298a2baded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_wistleblowing.to_csv(\"WBDatasetstec_NoDuplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2a50d-a6dc-475d-aaed-c431915ada4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_wistleblowing[\"title\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5fa7f-b356-4f14-afa5-a978090a7697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code section of merging all the csvs files containing posts collected in batches from the various subreddits\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "directory_path = '/Users/abi94/Documents/Sample/'\n",
    "\n",
    "file_pattern = '*.csv'\n",
    "\n",
    "file_list = glob.glob(directory_path + file_pattern)\n",
    "\n",
    "print(file_list)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('CommentsSubredTechAndTechnews.csv', index=False)\n",
    "\n",
    "print(\"Merged CSV files saved as 'CommentsSubredTechAndTechnews.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e7022-1544-4338-b262-79a12b7e27e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WBlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c051-7878-473e-8c66-33366fbe6338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code section to collect all the comments from the posts retrieved using the postIDs\n",
    "\n",
    "dfComment = []\n",
    "\n",
    "csvfilepath = 'WhistleblowingDatasetPosts.csv' #Dataset containing the posts collected previously\n",
    "\n",
    "def calccommentcontroversiality(comment):\n",
    "    totalvotes = comment.ups + comment.downs\n",
    "    score = comment.score\n",
    "    return totalvotes / max(totalvotes, score, 1)\n",
    "\n",
    "with open(csvfilepath, 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        postid = row[0]\n",
    "        posttitle = row[1]\n",
    "        try:\n",
    "            submission = reddit.submission(id=postid)\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                if comment.depth == 0:\n",
    "                    print(postid)\n",
    "                    controversialitycomment = calccommentcontroversiality(comment)\n",
    "                    dfComment.append([postid,posttitle,comment.id,comment.body,comment.score,comment.ups,comment.downs,controversialitycomment,comment.author,comment.subreddit,str(datetime.fromtimestamp(comment.created_utc))])                   \n",
    "                \n",
    "        except praw.exceptions.APIException as e:\n",
    "            if e.error_type == 'RATELIMIT':\n",
    "                print(\"Rate limited. Waiting before retrying...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "        except praw.exceptions.ClientException as e:\n",
    "            print(f\"Error occurred while processing post {postid}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing post {postid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "b50f1497-42b8-4d05-8999-304546e72e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResDFrame = pd.DataFrame(dfComment,columns=['PostID','Posttitle','CommentID','body','score','ups','downs','comment_controversiality','author','subreddit','comment_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022097d-abb8-4f56-b5c1-c5533ef59bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResDFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "8de1231d-3bfb-41e2-83d0-44baa7b3e469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ResDFrame.to_csv(\"CommentsWB_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
